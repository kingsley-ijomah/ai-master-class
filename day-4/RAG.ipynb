{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lPydXQrrJxG9"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install required packages\n",
        "# llama-index: The main library for data parsing and vector indexing\n",
        "# pymongo: MongoDB Python driver to connect to MongoDB Atlas from Colab\n",
        "!pip install llama-index pymongo\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-llms-openrouter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary module to mount Google Drive in Colab\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access its contents in your Colab environment\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the desired input directory path, using a subfolder for organization\n",
        "input_dir = \"/content/drive/My Drive/Colab Notebooks/RAG\"\n",
        "\n",
        "# Check if the directory exists, and create it if it does not\n",
        "if not os.path.isdir(input_dir):\n",
        "    os.makedirs(input_dir)\n",
        "    print(f\"Created directory: {input_dir}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {input_dir}\")\n"
      ],
      "metadata": {
        "id": "bDdXRaZ2P8up",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import HierarchicalNodeParser\n",
        "\n",
        "# Load all files recursively from input_dir and subfolders\n",
        "reader = SimpleDirectoryReader(input_dir=input_dir, recursive=True)\n",
        "documents = reader.load_data()\n",
        "\n",
        "# Initialize hierarchical parser (can customize chunk sizes, etc.)\n",
        "node_parser = HierarchicalNodeParser.from_defaults(\n",
        "    chunk_sizes=[2048, 512, 128],\n",
        "    chunk_overlap=40 # Overlap each chunk by 20 tokens to preserve context\n",
        ")\n",
        "\n",
        "# Parse documents into hierarchical chunks preserving structure\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# nodes now contain sophisticatedly parsed chunks with hierarchy info\n",
        "print(f\"Parsed {len(nodes)} hierarchical document chunks.\")\n"
      ],
      "metadata": {
        "id": "vXZ6ihAqI6Jk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required local embedding model\n",
        "# from llama_index.core import GPTVectorStoreIndex\n",
        "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "\n",
        "# # Set up a local embedding model using Hugging Face transformers (free to use)\n",
        "# embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "open_router_api=userdata.get('open_router_api')\n",
        "\n",
        "client = OpenAI(\n",
        "  base_url=\"https://openrouter.ai/api/v1\",\n",
        "  api_key=open_router_api,\n",
        ")\n",
        "\n",
        "class OpenRouterEmbeddingWrapper:\n",
        "    def __init__(self, client, model_name=\"openai/text-embedding-3-large\"):\n",
        "        self.client = client\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def get_text_embedding(self, text):\n",
        "        response = self.client.embeddings.create(\n",
        "            model=self.model_name,\n",
        "            input=text,\n",
        "            encoding_format=\"float\"\n",
        "        )\n",
        "        return response.data[0].embedding\n",
        "\n",
        "embed_model = OpenRouterEmbeddingWrapper(client)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cGNNSJA1SwS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import certifi\n",
        "from pymongo import MongoClient\n",
        "from google.colab import userdata\n",
        "\n",
        "db_user = userdata.get('db_user')\n",
        "db_pass = userdata.get('db_pass')\n",
        "db_name = userdata.get('db_name')\n",
        "\n",
        "# MongoDB connection (replace with your credentials)\n",
        "mongo_uri = f\"mongodb+srv://{db_user}:{db_pass}@ai-master.w5go1ll.mongodb.net/?appName={db_name}\"\n",
        "\n",
        "# Initialize the MongoDB client and get the database instance\n",
        "client = MongoClient(mongo_uri, tls=True, tlsCAFile=certifi.where())\n",
        "db = client[db_name]\n",
        "\n",
        "# Test the connection by listing collection names or pinging\n",
        "try:\n",
        "    # Option 1: List collections (will raise error if no connection)\n",
        "    collections = db.list_collection_names()\n",
        "    print(\"Connection test successful: Collections found:\", collections)\n",
        "except Exception as e:\n",
        "    print(\"Connection test failed:\", e)"
      ],
      "metadata": {
        "id": "iKVGd-v_IAFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings for nodes and save to MongoDB Atlas\n",
        "\n",
        "# Define your target database and collection\n",
        "collection = db[\"documents_with_embeddings\"]\n",
        "\n",
        "# traverse and save each node\n",
        "for node in nodes:\n",
        "    text = node.get_text().strip()\n",
        "    if text == \"\":\n",
        "        continue   # skip empty chunks\n",
        "    embedding = embed_model.get_text_embedding(node.text)\n",
        "    mongo_doc = {\n",
        "        \"doc_id\": node.ref_doc_id,\n",
        "        \"node_id\": node.node_id,\n",
        "        \"text\": node.get_text(),\n",
        "        \"embedding\": embedding,\n",
        "        \"metadata\": node.metadata\n",
        "    }\n",
        "    collection.update_one(\n",
        "        {\"node_id\": node.node_id},\n",
        "        {\"$set\": mongo_doc},\n",
        "        upsert=True\n",
        "    )\n",
        "\n",
        "print(f\"Generated embeddings and saved {len(nodes)} chunks to MongoDB.\")\n"
      ],
      "metadata": {
        "id": "YOYRSAreM_Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary classes for MongoDB Atlas vector search index creation\n",
        "from pymongo.operations import SearchIndexModel\n",
        "\n",
        "# Define the vector search index\n",
        "search_index_definition = {\n",
        "    \"mappings\": {\n",
        "        \"dynamic\": False,  # Disable dynamic schema mapping for safety\n",
        "        \"fields\": {\n",
        "            \"embedding\": {\n",
        "                \"type\": \"knnVector\",        # Must be knnVector for vector search\n",
        "                \"dimensions\": 3072,          # Match embedding vector dimension (adjust if different)\n",
        "                \"similarity\": \"cosine\"      # Similarity metric (cosine is common for embeddings)\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "index_name = \"docs_vector_index\"\n",
        "\n",
        "# Check if the index already exists to avoid duplicates (idempotent creation)\n",
        "try:\n",
        "    existing_indexes = collection.list_search_indexes()\n",
        "    existing_names = [idx[\"name\"] for idx in existing_indexes]\n",
        "\n",
        "    if index_name in existing_names:\n",
        "        print(f\"Search index '{index_name}' already exists, skipping creation.\")\n",
        "    else:\n",
        "        # Create the new vector search index\n",
        "        search_index_model = SearchIndexModel(\n",
        "            definition=search_index_definition,\n",
        "            name=index_name,\n",
        "            type=\"search\"\n",
        "        )\n",
        "        result = collection.create_search_index(model=search_index_model)\n",
        "        print(f\"Created vector search index '{index_name}': {result}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error checking or creating search index:\")\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "Pu6KnV2VfjVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the query text\n",
        "# How many productivity tips are listed in the document?\n",
        "# What is the book’s approach to planning your day?\n",
        "# What does the book say about email management?\n",
        "query_text = \"What is the summary of this doc\"\n",
        "\n",
        "# Generate the embedding for your query\n",
        "query_embedding = embed_model.get_text_embedding(query_text)\n",
        "\n",
        "# Define the aggregation pipeline using Atlas vector search\n",
        "search_pipeline = [\n",
        "    {\n",
        "        \"$search\": {\n",
        "            \"index\": index_name,\n",
        "            \"knnBeta\": {\n",
        "                \"vector\": query_embedding,   # Query vector for similarity search\n",
        "                \"path\": \"embedding\",         # Field containing stored embeddings\n",
        "                \"k\": 3                       # Number of nearest neighbors to retrieve\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\"$limit\": 3}  # Limit results to top 3\n",
        "]\n",
        "\n",
        "# Execute the aggregation query\n",
        "results = list(collection.aggregate(search_pipeline))\n",
        "\n",
        "# Print the texts of the top matching documents\n",
        "response = []\n",
        "for result in results:\n",
        "    response.append(result[\"text\"])\n",
        "    print(result[\"text\"])\n",
        "    print(\"----\")"
      ],
      "metadata": {
        "id": "jbtH25MsBEaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openrouter import OpenRouter\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "open_router_api=userdata.get('open_router_api')\n",
        "\n",
        "llm = OpenRouter(\n",
        "    api_key=open_router_api,\n",
        "    max_tokens=256,\n",
        "    context_window=4096,\n",
        "    model=\"gpt-4\",\n",
        ")"
      ],
      "metadata": {
        "id": "QdBYTtkHqMyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the response through LLM so it is presented back in a NL format\n",
        "if response:\n",
        "    # System instruction stays as before\n",
        "    system_msg = ChatMessage(\n",
        "        role=\"system\",\n",
        "        content=\"You are a helpful assistant. Use the provided context to answer the user's question as clearly and thoroughly as possible.\"\n",
        "    )\n",
        "    # User message asks for JSON structure\n",
        "    user_msg = ChatMessage(\n",
        "        role=\"user\",\n",
        "        content=f\"\"\"Context:\n",
        "{response}\n",
        "\n",
        "Question: {query_text}\n",
        "\n",
        "Please answer the question using only the provided context, and return your answer strictly in the following JSON format:\n",
        "\n",
        "{{\n",
        "  \"answer\": <your answer as a string>\n",
        "}}\n",
        "Only return valid JSON—do not add commentary or extra explanation outside the JSON object.\n",
        "\"\"\"\n",
        "    )\n",
        "    chat_history = [system_msg, user_msg]\n",
        "else:\n",
        "    print(\"No matching documents found.\")\n",
        "\n",
        "resp = llm.chat(chat_history)\n",
        "print(resp)\n"
      ],
      "metadata": {
        "id": "QD6TYYSxrZhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_record_index = 20 # Starting from the first record\n",
        "num_records_to_fetch = 25 # Fetch 5 records\n",
        "\n",
        "# Query MongoDB to get a range of records\n",
        "# The skip() method is used to skip a specified number of documents,\n",
        "# and the limit() method is used to restrict the number of documents returned.\n",
        "records_cursor = collection.find({}).skip(start_record_index).limit(num_records_to_fetch)\n",
        "\n",
        "print(f\"Fetching records from index {start_record_index} to {start_record_index + num_records_to_fetch - 1}:\")\n",
        "for i, record in enumerate(records_cursor):\n",
        "    print(f\"Record {start_record_index + i}:\")\n",
        "    print(f\"  Node ID: {record.get('node_id')}\")\n",
        "    print(f\"  Text: {record.get('text')}...\")\n",
        "    print(\"------------------\")"
      ],
      "metadata": {
        "id": "Vj6ZGMa62Ft5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}